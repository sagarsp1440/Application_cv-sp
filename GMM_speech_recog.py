# -*- coding: utf-8 -*-
"""Untitled4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LlryRL-m7mhGEppanaF-cp16k24T5Xgr
"""

from google.colab import drive
drive.mount('/content/drive')

pip install pydub

import numpy as np
import librosa
from pydub import AudioSegment
from pydub.utils import mediainfo
from sklearn import preprocessing

def mfcc_extraction(audio_filename,hop_duration, num_mfcc ):
  speech = AudioSegment.from_wav(audio_filename) #Read audio data from file
  samples = speech.get_array_of_samples()
  sampling_rate = speech.frame_rate #sampling rate f
  mfcc = librosa.feature.mfcc(np.float32(samples),sr = sampling_rate,hop_length = int(sampling_rate * hop_duration),n_mfcc = num_mfcc)
  return mfcc.T

from sklearn.mixture import GaussianMixture
def learningGMM(features, n_components,max_iter):
  gmm = GaussianMixture(n_components = n_components, max_iter = max_iter)
  gmm.fit(features)
  return gmm

import os
path = '/content/drive/My Drive/SpeakerData/'

speakers = os.listdir(path + 'Train/')
print(speakers)

import os
path = '/content/drive/My Drive/SpeakerData/'

speakers1 = os.listdir(path + 'Test/')
print(speakers1)

from sklearn import preprocessing
#this list is used to store the MFCC features of all training data of all speakers
mfcc_all_speakers = []
hop_duration = 0.015 #15ms
num_mfcc = 12

for s in speakers:
  sub_path = path + 'Train/' + s + '/'
  sub_file_names = [os.path.join(sub_path, f) for f in os.listdir(sub_path)]
  mfcc_one_speaker = np.asarray(())
  for fn in sub_file_names:
    mfcc_one_file = mfcc_extraction(fn, hop_duration, num_mfcc)
    if mfcc_one_speaker.size == 0:
      mfcc_one_speaker = mfcc_one_file
    else:
      mfcc_one_speaker = np.vstack((mfcc_one_speaker, mfcc_one_file))
  mfcc_all_speakers.append(mfcc_one_speaker)

import pickle
for i in range(0, len(speakers)):
  with open('/content/drive/My Drive/TrainingFeatures/' + speakers[i] + '_mfcc.fea','wb') as f:
    pickle.dump(mfcc_all_speakers[i], f)

n_components = 5
max_iter = 50
gmms = [] #list of GMMs, each is for a speaker
for i in range(0, len(speakers)):
  gmm = learningGMM(mfcc_all_speakers[i],n_components,max_iter)
  gmms.append(gmm)

for i in range(len(speakers)):
  with open('/content/drive/My Drive/Models/' + speakers[i] + '.gmm', 'wb') as f: #'wb' is for binary write
    pickle.dump(gmms[i], f)

gmms = []
for i in range(len(speakers)):
  with open('/content/drive/My Drive/Models/' + speakers[i] + '.gmm', 'rb') as f: #'wb' is for binary write
    gmm = pickle.load(f)
    gmms.append(gmm)

likelihood=np.zeros(len(gmms))
def speaker_recognition(audio_file_name, gmms):
  f= mfcc_extraction(audio_file_name, hop_duration,num_mfcc)
  for i in range(len(gmms)):
    score = gmms[i].score(f)
    likelihood[i]=score.sum()
    
  speaker_id = np.argmax(likelihood)
  print(np.max(score))
  return speaker_id

test_file_names=[]
test_labels=[]
for s1 in speakers1:
  sub_path_1 = path + 'Test/' + s1 + '/'
  sub_file_names_1 = [os.path.join(sub_path_1, f) for f in os.listdir(sub_path_1)]
  test_file_names += sub_file_names_1
  test_labels += [s1] * len(sub_file_names_1)
print(test_file_names)
print(test_labels)

train_file_names=[]
train_labels = []

for s in speakers:
  sub_path = path + 'Train/' + s + '/'
  sub_file_names = [os.path.join(sub_path, f) for f in os.listdir(sub_path)]
  train_file_names += sub_file_names
  train_labels += [s] * len(sub_file_names)


print(train_file_names)
print(train_labels)

from sklearn import svm
from sklearn.ensemble import AdaBoostClassifier
from sklearn.metrics import classification_report, confusion_matrix

for i  in test_file_names:
  speaker_id = speaker_recognition(i, gmms)
  print(speakers[speaker_id])

train_s = []
test_s = []
for filename in train_file_names:
  train_s.append(speaker_recognition(filename, gmms))
for filename in test_file_names:
  test_s.append(speaker_recognition(filename, gmms))
train_s=np.array(train_s)
test_s=np.array(test_s)
train_s=train_s.reshape(-1,1)
test_s=test_s.reshape(-1,1)
svm_1=svm.SVC()
svm_1.fit(train_s,train_labels)

pred_labels = svm_1.predict(test_s)
print('SVM report:', np.sum(pred_labels ==test_labels)/len(test_labels))


ada= AdaBoostClassifier()
ada.fit(train_s,train_labels)

pred_labels1 = ada.predict(test_s)
print('AdaBoost report:', np.sum(pred_labels1 ==test_labels)/len(test_labels))





